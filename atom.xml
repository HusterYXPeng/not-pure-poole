<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZhangFeng&#39;s Blog</title>
  
  <subtitle>HuaZhong University Of Science And Technology</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-10-18T16:15:25.393Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ZhangLaplace</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Caffe Blob分析</title>
    <link href="http://yoursite.com/2017/10/18/Caffe_blob/"/>
    <id>http://yoursite.com/2017/10/18/Caffe_blob/</id>
    <published>2017-10-17T16:03:10.000Z</published>
    <updated>2017-10-18T16:15:25.393Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Caffe-blob"><a href="#Caffe-blob" class="headerlink" title="Caffe_blob"></a>Caffe_blob</h1><h3 id="1-基本数据结构"><a href="#1-基本数据结构" class="headerlink" title="1.基本数据结构"></a>1.基本数据结构</h3><p>  Blob为模板类，可以理解为四维数组，n <em> c </em> h * w的结构,layer内为输入data和diff，<br>net间的blob为学习的参数.内部封装了SyncedMemory类。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">protected</span>:</div><div class="line">    <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; data_; <span class="comment">// data指针</span></div><div class="line">    <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; diff_; <span class="comment">// diff指针</span></div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_; <span class="comment">// blob形状</span></div><div class="line">    <span class="keyword">int</span> count_; <span class="comment">// blob的nchw</span></div><div class="line">    <span class="keyword">int</span> capacity_; <span class="comment">// 当前的Blob容量，当Blob reshape后count&gt; capacity_时，capacity_ = count_; 重新new 然后 reset data和 diff</span></div></pre></td></tr></table></figure></p><a id="more"></a><h3 id="2-常用函数"><a href="#2-常用函数" class="headerlink" title="2.常用函数"></a>2.常用函数</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">Blob&lt;<span class="keyword">float</span>&gt;test;</div><div class="line">test.shape_string();<span class="comment">//初始为空 0 0 0 0</span></div><div class="line">test.Reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>);<span class="comment">// shape_string() 1,2,3,4</span></div><div class="line">test.count(<span class="keyword">int</span> start_axis,<span class="keyword">int</span> end_axis); <span class="comment">// start_axis---end_axis .x* shape[i]</span></div><div class="line">test.count();<span class="comment">// nchw  count(1) chw count(2) hw.....</span></div><div class="line"><span class="keyword">const</span> <span class="keyword">float</span>* data = test.cpu_data();</div><div class="line"><span class="keyword">const</span> <span class="keyword">float</span>* diff = test.cpu_diff();</div><div class="line"><span class="keyword">float</span>* data_1 = test.mutable_cpu_data();<span class="comment">//mutable修饰的表示可以修改内部值</span></div><div class="line"><span class="keyword">float</span>* diff_1 = test.mutable_cpu_diff();</div><div class="line">test.asum_data();<span class="comment">//求和</span></div><div class="line">test.sumsq_data();<span class="comment">//平方和</span></div><div class="line">test.Update();<span class="comment">//data = data-diff;</span></div><div class="line">a.ToProto(BlobProto&amp; bp,<span class="literal">true</span>/<span class="literal">false</span>);<span class="comment">//(FromProto)</span></div><div class="line"><span class="keyword">int</span> index = a.CanonicalAxisIndex(<span class="keyword">int</span> axis_index);<span class="comment">// if &lt; 0 ,return num_axis()+axis_index;//索引序列</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">offset</span><span class="params">(n,c,h,w)</span></span>;<span class="comment">//((n*channels()+c)*height()+h)*width()+w</span></div><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">data_at</span><span class="params">(n,c,h,w)</span></span>;<span class="comment">//return cpu_data()[offset(n,c,h,w)];</span></div><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">diff_at</span><span class="params">(n,c,h,w)</span></span>;<span class="comment">//return cpu_diff()[offset(n,c,h,w)];</span></div><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt;&amp; data() <span class="keyword">const</span>&#123;<span class="keyword">return</span> _data&#125;;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">scale_data</span><span class="params">(Dtype scale_factor)</span></span>;<span class="comment">// data乘以一个标量。同理 scale_diff();</span></div></pre></td></tr></table></figure><h3 id="3-写入磁盘操作"><a href="#3-写入磁盘操作" class="headerlink" title="3.写入磁盘操作"></a>3.写入磁盘操作</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//Blob内部值写入到磁盘</span></div><div class="line">Blob&lt;<span class="keyword">float</span>&gt;a;</div><div class="line">a.Reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>);</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> count = a.count();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">    a[i] = i;<span class="comment">//init the test Blob</span></div><div class="line">&#125;</div><div class="line">BlobProto bp,bp2;</div><div class="line">a.ToProto(&amp;bp,<span class="literal">true</span>);<span class="comment">//写入data和diff到bp中</span></div><div class="line">WriteProtoToBinaryFile(bp,<span class="string">"a.blob"</span>);<span class="comment">//写入磁盘</span></div><div class="line">ReadProtoFromBinaryFile(<span class="string">"a.blob"</span>,&amp;bp2);<span class="comment">//从磁盘读取blob</span></div><div class="line">Blob&lt;<span class="keyword">float</span>&gt;b;</div><div class="line">b.FromProto(bp2,<span class="literal">true</span>);<span class="comment">//序列化对象bp2中克隆b，完整克隆</span></div><div class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> n = <span class="number">0</span>; n &lt; b.num(); n++) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> c = <span class="number">0</span>; c &lt; b.channels(); c++) &#123;</div><div class="line">       <span class="keyword">for</span> (<span class="keyword">size_t</span> h = <span class="number">0</span>; h &lt; b.height(); h++) &#123;</div><div class="line">           <span class="keyword">for</span> (<span class="keyword">size_t</span> w = <span class="number">0</span>; w &lt; b.width(); w++) &#123;</div><div class="line">               <span class="built_in">cout</span>&lt;&lt;<span class="string">"b["</span>&lt;&lt;n&lt;&lt;<span class="string">"]["</span>&lt;&lt;c&lt;&lt;<span class="string">"]["</span>&lt;&lt;h&lt;&lt;<span class="string">"]["</span>&lt;&lt;w&lt;&lt;<span class="string">"]["</span>&lt;&lt;w&lt;&lt;<span class="string">"]="</span>&lt;&lt;b[(((n*b.channels()+c)*b.height)+h)*b.width()+w]&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">               <span class="comment">//(((n*c+ci)*h+hi)*w+wi)</span></div><div class="line">           &#125;</div><div class="line">       &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Caffe-blob&quot;&gt;&lt;a href=&quot;#Caffe-blob&quot; class=&quot;headerlink&quot; title=&quot;Caffe_blob&quot;&gt;&lt;/a&gt;Caffe_blob&lt;/h1&gt;&lt;h3 id=&quot;1-基本数据结构&quot;&gt;&lt;a href=&quot;#1-基本数据结构&quot; class=&quot;headerlink&quot; title=&quot;1.基本数据结构&quot;&gt;&lt;/a&gt;1.基本数据结构&lt;/h3&gt;&lt;p&gt;  Blob为模板类，可以理解为四维数组，n &lt;em&gt; c &lt;/em&gt; h * w的结构,layer内为输入data和diff，&lt;br&gt;net间的blob为学习的参数.内部封装了SyncedMemory类。&lt;br&gt;&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;protected&lt;/span&gt;:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;shared_ptr&lt;/span&gt;&amp;lt;SyncedMemory&amp;gt; data_; &lt;span class=&quot;comment&quot;&gt;// data指针&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;shared_ptr&lt;/span&gt;&amp;lt;SyncedMemory&amp;gt; diff_; &lt;span class=&quot;comment&quot;&gt;// diff指针&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;vector&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; shape_; &lt;span class=&quot;comment&quot;&gt;// blob形状&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; count_; &lt;span class=&quot;comment&quot;&gt;// blob的nchw&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; capacity_; &lt;span class=&quot;comment&quot;&gt;// 当前的Blob容量，当Blob reshape后count&amp;gt; capacity_时，capacity_ = count_; 重新new 然后 reset data和 diff&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Caffe" scheme="http://yoursite.com/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="http://yoursite.com/tags/Caffe/"/>
    
      <category term="DeepLearning" scheme="http://yoursite.com/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法 1-统计学习算法概述</title>
    <link href="http://yoursite.com/2017/09/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%951-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"/>
    <id>http://yoursite.com/2017/09/14/统计学习方法1-统计学习算法概述/</id>
    <published>2017-09-14T06:24:11.000Z</published>
    <updated>2017-10-18T16:07:11.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="统计学习的主要特点"><a href="#统计学习的主要特点" class="headerlink" title="统计学习的主要特点"></a>统计学习的主要特点</h2><pre><code>统计学习的对象是数据，目的是对数据进行预测与分析，特别是对未知数据进行预测与分析。</code></pre><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><pre><code>监督学习(supervised learning)无监督学习(unsupervised learning)半监督学习(self-supervised learning)增强式学习(reinfoucement learning)</code></pre><a id="more"></a><h3 id="监督学习-supervised-learning"><a href="#监督学习-supervised-learning" class="headerlink" title="监督学习(supervised learning)"></a>监督学习(supervised learning)</h3><p>输入实际x的特征向量记做$x = (x^{(1)},x^{(2)},x^{(3)}, \cdots ,x^{(n)})^T$<br>训练集：$T={(x_1,y_1),(x_2,y_2),(x_3,y_3),\cdots (x_n,y_n)}$<br>输入变量与输出变量均为连续变量的预测问题为回归问题；<br>输出变量为有限个离散变量的预测问题为分类问题；</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;统计学习的主要特点&quot;&gt;&lt;a href=&quot;#统计学习的主要特点&quot; class=&quot;headerlink&quot; title=&quot;统计学习的主要特点&quot;&gt;&lt;/a&gt;统计学习的主要特点&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;统计学习的对象是数据，目的是对数据进行预测与分析，特别是对未知数据进行预测与分析。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;分类&quot;&gt;&lt;a href=&quot;#分类&quot; class=&quot;headerlink&quot; title=&quot;分类&quot;&gt;&lt;/a&gt;分类&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;监督学习(supervised learning)
无监督学习(unsupervised learning)
半监督学习(self-supervised learning)
增强式学习(reinfoucement learning)
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Linear Regression</title>
    <link href="http://yoursite.com/2017/09/10/Linear-Regression/"/>
    <id>http://yoursite.com/2017/09/10/Linear-Regression/</id>
    <published>2017-09-10T02:23:08.000Z</published>
    <updated>2017-10-18T16:06:37.473Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Model-and-Cost-Function-模型和损失函数"><a href="#Model-and-Cost-Function-模型和损失函数" class="headerlink" title="Model and Cost Function(模型和损失函数)"></a>Model and Cost Function(模型和损失函数)</h2><p>对于model，给出如下定义 $y = \theta x$<br>损失函数$J(\theta ): minimize\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i)-y^i)^2$<br><a id="more"></a><br>Gradient descent algorithm<br>repeat until convergence{<br>    $\quad \theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta)$<br>}</p><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p>寻找两类样本正中间的划分超平面，因为该超平面对训练样本的布局扰动的容忍度最好，是最鲁棒的<br>划分超平面方程:<br>$$wx+b = 0$$<br>我们假使<br>$$<br>\begin{cases}<br>wx_i+b &gt;= 1 \qquad\quad y_i = +1 \\\<br>\\<br>wx_i+b &lt;=-1 \qquad\, y_i = -1<br>\end{cases}<br>$$<br>则距离超平面最近的几个点使得下列式子成立<br>$$\max\limits_{w,b}(\frac{2}{||w||}) \rightarrow \min_{w,b}\frac{1}{2}||w||^2$$<br>$$s.t. y_i(wx_i+b)\ge 1 i = 1,2,…,m.$$<br>通用表达式:<br>    $f(x)=w\psi(x)+b = \sum_{i=1}^{m}a_iy_i\psi(x_i)^T\psi(x)+b=\sum_{i=1}^{m}a_iy_i\kappa(x,x_i)+b$<br>$\kappa 为核函数.$</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Model-and-Cost-Function-模型和损失函数&quot;&gt;&lt;a href=&quot;#Model-and-Cost-Function-模型和损失函数&quot; class=&quot;headerlink&quot; title=&quot;Model and Cost Function(模型和损失函数)&quot;&gt;&lt;/a&gt;Model and Cost Function(模型和损失函数)&lt;/h2&gt;&lt;p&gt;对于model，给出如下定义 $y = \theta x$&lt;br&gt;损失函数$J(\theta ): minimize\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i)-y^i)^2$&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>C++随笔</title>
    <link href="http://yoursite.com/2017/09/08/C++%E9%9A%8F%E7%AC%94/"/>
    <id>http://yoursite.com/2017/09/08/C++随笔/</id>
    <published>2017-09-08T05:44:33.000Z</published>
    <updated>2017-09-08T15:32:47.675Z</updated>
    
    <content type="html"><![CDATA[<h3 id="重写，重定义、重载的区别"><a href="#重写，重定义、重载的区别" class="headerlink" title="重写，重定义、重载的区别"></a>重写，重定义、重载的区别</h3><a id="more"></a><h4 id="重写"><a href="#重写" class="headerlink" title="重写"></a>重写</h4><p>$\qquad 子类(派生类)重新定义基类的虚函数方法，要求函数名，函数参数，返回类型完全相同.并\\$<br>$且基于必须是虚函数，不能有static关键字,重写函数的访问修饰符可以与基类的不同。\\$<br>$\qquad 基类指针指向派生类，若实现了重写，则调用派生类，若没，则调用基类,即实现多态$</p><h4 id="重定义"><a href="#重定义" class="headerlink" title="重定义"></a>重定义</h4><p>$\qquad 子类(派生类)重新申明和定义基类的函数，要求函数名相同，但是返回值可以不同，参数\\$<br>$不同，无论有无virtual，基类的都将被隐藏，参数相同，基类如果没有virtual，则基类的函被\\$<br>$隐藏$</p><h4 id="重载"><a href="#重载" class="headerlink" title="重载"></a>重载</h4><p>$\qquad函数名相同，但是他们的参数列表个数或者顺序，类型不同，且不能仅有返回类型不同，要\\$<br>$求再同一个作用于.$</p><h3 id="多态的实现方式"><a href="#多态的实现方式" class="headerlink" title="多态的实现方式"></a>多态的实现方式</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>$\qquad 多态: 即程序运行中，系统根据对象指针所指向的类别对相同的消息进行不同的方法处理$</p><h4 id="动态多态"><a href="#动态多态" class="headerlink" title="动态多态"></a>动态多态</h4><p>$\qquad 通过类的继承和虚函数机制，在程序运行期实现多态,虚函数表$</p><h4 id="静态多态"><a href="#静态多态" class="headerlink" title="静态多态"></a>静态多态</h4><p>$\qquad 函数重载；运算符重载$</p><h3 id="常用排序算法"><a href="#常用排序算法" class="headerlink" title="常用排序算法"></a>常用排序算法</h3><h4 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h4><p>$\qquad 快速排序的实现:$<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span> a[],<span class="keyword">int</span> l ,<span class="keyword">int</span> r)</span></span>&#123;<span class="comment">//或者vector</span></div><div class="line">    <span class="keyword">if</span>(l &lt; r)&#123;</div><div class="line">        <span class="keyword">int</span> i = l ,j = r ;</div><div class="line">        <span class="keyword">int</span> sed = a[i];<span class="comment">//种子点</span></div><div class="line">        <span class="keyword">while</span>(i &lt; j )&#123;</div><div class="line">            <span class="keyword">while</span>(i &lt; j &amp;&amp; a[j] &gt; sed )</div><div class="line">                --j;</div><div class="line">            <span class="keyword">if</span>(i &lt; j)</div><div class="line">                a[i++] = a[j];</div><div class="line">            <span class="keyword">while</span>(i &lt; j &amp;&amp; a[i] &lt; sed )</div><div class="line">                ++i;</div><div class="line">            <span class="keyword">if</span>(i &lt; j)</div><div class="line">                a[j--] = a[i];</div><div class="line">        &#125;</div><div class="line">        a[i] = sed;</div><div class="line">        quickSort(a,l,i<span class="number">-1</span>);</div><div class="line">        qucikSort(a,i+<span class="number">1</span>,r);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;重写，重定义、重载的区别&quot;&gt;&lt;a href=&quot;#重写，重定义、重载的区别&quot; class=&quot;headerlink&quot; title=&quot;重写，重定义、重载的区别&quot;&gt;&lt;/a&gt;重写，重定义、重载的区别&lt;/h3&gt;
    
    </summary>
    
      <category term="C++" scheme="http://yoursite.com/categories/C/"/>
    
    
      <category term="C++" scheme="http://yoursite.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Logistic回归分析</title>
    <link href="http://yoursite.com/2017/09/07/logistic/"/>
    <id>http://yoursite.com/2017/09/07/logistic/</id>
    <published>2017-09-07T12:03:49.000Z</published>
    <updated>2017-09-08T07:27:34.852Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Logistic回归分析"><a href="#Logistic回归分析" class="headerlink" title="Logistic回归分析"></a>Logistic回归分析</h3><p>$\qquad Logistic回归为概率型非线性回归模型，机器学习常用的二分类分类器，其表达式为:$</p><p>$\quad \quad z=w_{1}*x_{1}+w_{2}*x_{2}+\cdots +w_{n}*x_{n}+b=\sum_{i=0}^n w_{i}x_{i}  (其中 b等于w_{0}，x_{0}等于1)则:$<br><a id="more"></a><br>$$f(x) = \frac{1}{1+exp(-z)}$$</p><p>$\quad \quad$即对于二分类，如果$f(x)\ge{0.5}$,则$x$属于第一类，即预测$y=1$，反之$x$属于第二类，预测$y=0$；样本的分布如下，其中，$C_1$表示第一个类别，$C_2$表示第二个类别，样本个数为$n$</p><p>$$trainingData \quad\, x^1 \quad\, x^2 \quad\, x^3 \quad\,\cdots \quad\, x^n $$</p><p>$\qquad \qquad \qquad \qquad \qquad \qquad labels \qquad   \quad  C_{1} \quad C_{1} \quad C_{2} \quad \cdots \quad C_{1} \\$<br>$\qquad$我们的目的是：对于类别为$1$的正样本$f_{w,b}(x)$ 尽可能大,而类别为$2$的负样本$f_{w,b}(x)$ 尽可能小,则我们需要最大化：$L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\cdots f_{w,b}(x^n)$来寻找最佳的$w$和$b$<br>$$<br>w^{*},b^{*} = arg\max\limits_{w,b}(L(w,b))\Longrightarrow w^{*},b^{*} = arg\min\limits_{w,b}(-ln{L(w,b)})<br>$$</p><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a><a href="https://zh.wikipedia.org/zh-hans/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95" target="_blank" rel="external">随机梯度下降法</a></h3><p>$\qquad 我们需要优化的函数:-ln{L(w,b)} = -{ln{f_{w,b}(x^1)}+lnf_{w,b}(x^2)+ln(1-f_{w,b}(x^3))+\cdots lnf_{w,b}(x^n)}\quad \\$<br>$$<br>\qquad 假设：<br>\begin{cases}<br>\hat{y} = 1 \qquad x\in1 \\\<br>\\<br>\hat{y} = 0 \qquad  x\in0<br>\end{cases}<br>\qquad 已知\,f(x) = \frac{1}{1+exp(-z)}\quad z = \sum_{i=0}^n  w_{i}x_{i} 则<br>$$<br>$\qquad 我们需要优化的函数简化为：ln{L(w,b)} =\sum_{j=1}^{n}{\hat{y}^j\,lnf_{w,b}(x^j)+(1-\hat{y}^j)\,ln(1-f_{w,b}(x^j))} \\$</p><p>$\qquad 当\,\,\hat{y}=1时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = lnf_{w,b}(x) \\$<br>$\qquad 当\,\,\hat{y}=0时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = ln(1-f_{w,b}(x)) \qquad \\$<br>$\qquad 即均满足上式 , 因此:$</p><p>$\qquad \qquad \quad \frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\$</p><p>$\qquad \quad \quad 而 \, \frac{\partial lnf_{w,b}(x)}{\partial w_i}=\frac{\partial lnf_{w,b}(x)}{\partial z}*\frac{\partial z}{\partial w_i} \\$</p><p>$\qquad \qquad \qquad \qquad \quad=\frac{1}{f_{w,b}(x)}* \frac{\partial f_{w,b}(x)}{\partial z}*x_i \\$</p><p>$\qquad \qquad \qquad \qquad \quad=\frac{1}{f_{w,b}(x)}*f_{w,b}(x)*(1-f_{w,b}(x))*x_i \\$</p><p>$\qquad \qquad \qquad \qquad \quad=(1-f_{w,b}(x))*x_i \\$</p><p>$\quad \quad 同理 \quad   \frac{\partial (1-lnf_{w,b}(x))}{\partial w_i}=f_{w,b}(x)*x_i \qquad 则化简后:\\$<br>$\qquad \quad\,\, \qquad \frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\$</p><p>$\qquad \qquad \qquad \quad \qquad = \sum_{j=1}^{n}{\hat{y}^j(1-f_{w,b}(x^j))x^j_i+(1-\hat{y}^j)*f_{w,b}(x^j)x^j_i} \\$</p><p>$\qquad \qquad \quad\qquad \qquad = \sum_{j=1}^{n}(\hat{y}^j -f_{w,b}(x^j))x^j_i \\$</p><p>$\qquad b的推导与w的相似，可以得到w的更新迭代过程：w_{i} \leftarrow w_{i}-\alpha*\sum_{j=0}^{n}(\hat{y}^j-f_{w,b}(x^j))x^j_i \\$</p><p><img src="http://images2017.cnblogs.com/blog/888534/201709/888534-20170908103015851-1635753052.png" alt=""></p><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><h4 id="1-为什么选用-crossEntropy-损失函数，而不用L2损失函数"><a href="#1-为什么选用-crossEntropy-损失函数，而不用L2损失函数" class="headerlink" title="1. 为什么选用$crossEntropy$损失函数，而不用L2损失函数"></a>1. 为什么选用$crossEntropy$损失函数，而不用L2损失函数</h4><p>$答:logistic不像linear \,\, regression使用L2损失函数的原因，主要是由于logistic的funcion的形式，\\$<br>$由于sigmoid函数的存在，如果logistic采取L2 loss时，损失函数为：\\$<br>$$\frac{\partial (f_{w,b}(x)-\hat{y})^2}{\partial w_i}=2(f_{w,b}(x)-\hat{y})f_{w,b}(x)(1-f_{w,b}(x))x_i $$<br>$则当\,\hat{y}=1, f_{w,b}(x) = 1 \quad 预测为1 ，即预测完全正确时 \quad loss=0 \quad  \\$<br>$但是当\,\hat{y}=1,f_{w,b}(x) = 0 \quad 预测为0 ，即预测完全错误时 \quad loss却依然为0 \quad显然不对 \\$</p><h4 id="2-logistic-regression-的分类概率为什么选取了-sigmoid-函数"><a href="#2-logistic-regression-的分类概率为什么选取了-sigmoid-函数" class="headerlink" title="2. $logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数"></a>2. <a href="https://www.zhihu.com/question/54707359" target="_blank" rel="external">$logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数</a></h4><p>$答: 我们假设样本的分布服从二次高斯分布，即\\$</p><p>$f_{\mu,\Sigma}(x) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu)^T|\Sigma|^{-1}(x-\mu)},其中\mu为均值，\Sigma为协方差矩阵 \\$</p><p>$输入为x，输出f_{\mu,\Sigma}(x)为样本x的概率密度，高斯分布的形状分布取决于均值\mu和协方差矩阵\Sigma, \\$<br>$因此需要求取最佳的高斯分布来满足样本的分布 \\$</p><p>$$Maximum Likelihood : L(\mu,\Sigma) = f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)f_{\mu,\Sigma}(x^3)\cdots\cdots f_{\mu,\Sigma}(x^{N})$$<br>$$\mu^{*}，\Sigma^{*} = arg\max\limits_{\mu,\Sigma}L(\mu,\Sigma)$$<br>$$\mu^{*} = \frac{1}{N}\sum_{i=0}^{N}{x^i}$$<br>$$\Sigma^{*} = \frac{1}{N}\sum_{i=0}^{N}{(x^i-\mu^{*})(x^i-\mu^{*})^T}$$</p><p>$对于一个二分类，我们假设类别1的样本高斯分布的均值为\mu^1,类别2的样本的高斯分布均值为\mu^2,他们具有相同的协方差\Sigma \\$<br>$$\mu^1 = \sum_{i=1}^{n_1} x_i\qquad (x_i \in C_1) \quad ;\quad \mu^2 = \sum_{i=1}^{n_2} x_i\quad(x_i \in C_2) $$<br>$$\Sigma^1 = \sum_{i=1}^{n_1}(x_i-u^1)(x_i-u^1)^T ;\quad \Sigma^2 = \sum_{i=1}^{n_2}(x_i-u^2)(x_i-u^2)^T ;\quad \Sigma=\frac{n_1}{n_1+n_2}\Sigma^1+\frac{n_1}{n_1+n_2}\Sigma^2 $$</p><p>$对于样本x，如果属于C_1则有：\\$</p><p>$\qquad \qquad\qquad \qquad P(C_{1}|x) \,\,= \frac{P(C_{1},x)}{P(x)} \\$</p><p>$\qquad \qquad\qquad \qquad \qquad \qquad =\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{1})*P(C_{1})+P(x|C_{2})*P(C_{2})} \\$</p><p>$\qquad \qquad\qquad \qquad \qquad \qquad =\frac{1}{1+\frac{P(x|C_{2})P(C_{2})}{P(x|C_{1})P(C_{1})}} \\$</p><p>$\qquad \qquad\qquad \qquad \qquad \qquad =\frac{1}{1+exp(-\alpha)} \\$</p><p>$其中\,\, \alpha= \ln(\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{2})*P(C_{2})})$</p><p>$将P(x|C_i)带入高斯分布的公式:\\$<br>$$P(C_1)=\frac{n_1}{n_1+n_2}\quad , \quad P(C_2)=\frac{n_2}{n_1+n_2} $$<br>$$P(x|C_1) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)} $$<br>$$P(x|C_2) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2)} $$<br>$\alpha= lnP(x|C_1)-lnP(x|C_2)+ln\frac{P(C_1)}{P(C_2)} \\$<br>$\quad =-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)-(-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2))+ln\frac{n_1}{n_2}\\$<br>$\quad =-\frac{1}{2}x^T(\Sigma)^{-1}x+(u^1)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}x^T(\Sigma)^{-1}x-(u^2)^T(\Sigma)^{-1}x+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$<br>$\quad = (u^1-u^2)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$<br>$\quad = wx+b\\$<br>$\quad w = (u^1-u^2)^T(\Sigma)^{-1} \quad ; \quad b=-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$<br>$\quad 因此可以得到对于满足猜想的二次高斯分布的datasets，生成模型的分类表达式与logistic是一致的 \\$</p><h3 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h3><h4 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h4><pre><code>基于现有的样本，对样本分布做了一个猜测（极大似然），因此当数据集较少，或者有噪声的时候，</code></pre><p>都能达到一个较好的结果(不过分依赖于实际样本),并且可以根据不同的概率model完成样本分布的gauss</p><h4 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h4><pre><code>基于决策的方式（判别式），通过优化方法(sgd)寻找最优参数，对样本的依赖大，样本充足时，其</code></pre><p>效果一般比生成模型好(基于事实 not 基于猜测)</p><h3 id="小扩展"><a href="#小扩展" class="headerlink" title="小扩展"></a>小扩展</h3><h4 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h4><pre><code>基于先验概率得出的每个类别的后验概率为softmax函数，即：</code></pre><p>$\\$<br>$\qquad \qquad \qquad \qquad \, P(C_i|x) = \frac{P(x|C_i)P(C_i)}{\sum_{j=1}^{n}P(x|C_j)P(C_j)}\\$</p><p>$\qquad \qquad \qquad \qquad \qquad \qquad = \frac{exp(a_k)}{\sum_{j=1}^{n}a_j}\\$</p><h4 id="待续"><a href="#待续" class="headerlink" title="待续"></a>待续</h4><p>未完待续</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Logistic回归分析&quot;&gt;&lt;a href=&quot;#Logistic回归分析&quot; class=&quot;headerlink&quot; title=&quot;Logistic回归分析&quot;&gt;&lt;/a&gt;Logistic回归分析&lt;/h3&gt;&lt;p&gt;$\qquad Logistic回归为概率型非线性回归模型，机器学习常用的二分类分类器，其表达式为:$&lt;/p&gt;
&lt;p&gt;$\quad \quad z=w_{1}*x_{1}+w_{2}*x_{2}+\cdots +w_{n}*x_{n}+b=\sum_{i=0}^n w_{i}x_{i}  (其中 b等于w_{0}，x_{0}等于1)则:$&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成模型" scheme="http://yoursite.com/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
</feed>
