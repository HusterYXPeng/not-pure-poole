<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Caffe Blob分析]]></title>
    <url>%2F2017%2F10%2F18%2FCaffe_blob%2F</url>
    <content type="text"><![CDATA[Caffe_blob1.基本数据结构 Blob为模板类，可以理解为四维数组，n c h * w的结构,layer内为输入data和diff，net间的blob为学习的参数.内部封装了SyncedMemory类。123456protected: shared_ptr&lt;SyncedMemory&gt; data_; // data指针 shared_ptr&lt;SyncedMemory&gt; diff_; // diff指针 vector&lt;int&gt; shape_; // blob形状 int count_; // blob的nchw int capacity_; // 当前的Blob容量，当Blob reshape后count&gt; capacity_时，capacity_ = count_; 重新new 然后 reset data和 diff 2.常用函数12345678910111213141516171819Blob&lt;float&gt;test;test.shape_string();//初始为空 0 0 0 0test.Reshape(1,2,3,4);// shape_string() 1,2,3,4test.count(int start_axis,int end_axis); // start_axis---end_axis .x* shape[i]test.count();// nchw count(1) chw count(2) hw.....const float* data = test.cpu_data();const float* diff = test.cpu_diff();float* data_1 = test.mutable_cpu_data();//mutable修饰的表示可以修改内部值float* diff_1 = test.mutable_cpu_diff();test.asum_data();//求和test.sumsq_data();//平方和test.Update();//data = data-diff;a.ToProto(BlobProto&amp; bp,true/false);//(FromProto)int index = a.CanonicalAxisIndex(int axis_index);// if &lt; 0 ,return num_axis()+axis_index;//索引序列int offset(n,c,h,w);//((n*channels()+c)*height()+h)*width()+wfloat data_at(n,c,h,w);//return cpu_data()[offset(n,c,h,w)];float diff_at(n,c,h,w);//return cpu_diff()[offset(n,c,h,w)];inline const shared_ptr&lt;SyncedMemory&gt;&amp; data() const&#123;return _data&#125;;void scale_data(Dtype scale_factor);// data乘以一个标量。同理 scale_diff(); 3.写入磁盘操作1234567891011121314151617181920212223//Blob内部值写入到磁盘Blob&lt;float&gt;a;a.Reshape(1,2,3,4);const int count = a.count();for (size_t i = 0; i &lt; count; i++) &#123; a[i] = i;//init the test Blob&#125;BlobProto bp,bp2;a.ToProto(&amp;bp,true);//写入data和diff到bp中WriteProtoToBinaryFile(bp,"a.blob");//写入磁盘ReadProtoFromBinaryFile("a.blob",&amp;bp2);//从磁盘读取blobBlob&lt;float&gt;b;b.FromProto(bp2,true);//序列化对象bp2中克隆b，完整克隆for (size_t n = 0; n &lt; b.num(); n++) &#123; for (size_t c = 0; c &lt; b.channels(); c++) &#123; for (size_t h = 0; h &lt; b.height(); h++) &#123; for (size_t w = 0; w &lt; b.width(); w++) &#123; cout&lt;&lt;"b["&lt;&lt;n&lt;&lt;"]["&lt;&lt;c&lt;&lt;"]["&lt;&lt;h&lt;&lt;"]["&lt;&lt;w&lt;&lt;"]["&lt;&lt;w&lt;&lt;"]="&lt;&lt;b[(((n*b.channels()+c)*b.height)+h)*b.width()+w]&lt;&lt;endl; //(((n*c+ci)*h+hi)*w+wi) &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计学习方法 1-统计学习算法概述]]></title>
    <url>%2F2017%2F09%2F14%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%951-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[统计学习的主要特点统计学习的对象是数据，目的是对数据进行预测与分析，特别是对未知数据进行预测与分析。 分类监督学习(supervised learning) 无监督学习(unsupervised learning) 半监督学习(self-supervised learning) 增强式学习(reinfoucement learning) 监督学习(supervised learning)输入实际x的特征向量记做$x = (x^{(1)},x^{(2)},x^{(3)}, \cdots ,x^{(n)})^T$训练集：$T={(x_1,y_1),(x_2,y_2),(x_3,y_3),\cdots (x_n,y_n)}$输入变量与输出变量均为连续变量的预测问题为回归问题；输出变量为有限个离散变量的预测问题为分类问题；]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linear Regression]]></title>
    <url>%2F2017%2F09%2F10%2FLinear-Regression%2F</url>
    <content type="text"><![CDATA[Model and Cost Function(模型和损失函数)对于model，给出如下定义 $y = \theta x$损失函数$J(\theta ): minimize\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i)-y^i)^2$Gradient descent algorithmrepeat until convergence{ $\quad \theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta)$} SVM寻找两类样本正中间的划分超平面，因为该超平面对训练样本的布局扰动的容忍度最好，是最鲁棒的划分超平面方程:$$wx+b = 0$$我们假使$$\begin{cases}wx_i+b &gt;= 1 \qquad\quad y_i = +1 \\\\\wx_i+b &lt;=-1 \qquad\, y_i = -1\end{cases}$$则距离超平面最近的几个点使得下列式子成立$$\max\limits_{w,b}(\frac{2}{||w||}) \rightarrow \min_{w,b}\frac{1}{2}||w||^2$$$$s.t. y_i(wx_i+b)\ge 1 i = 1,2,…,m.$$通用表达式: $f(x)=w\psi(x)+b = \sum_{i=1}^{m}a_iy_i\psi(x_i)^T\psi(x)+b=\sum_{i=1}^{m}a_iy_i\kappa(x,x_i)+b$$\kappa 为核函数.$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++随笔]]></title>
    <url>%2F2017%2F09%2F08%2FC%2B%2B%E9%9A%8F%E7%AC%94%2F</url>
    <content type="text"><![CDATA[重写，重定义、重载的区别 重写$\qquad 子类(派生类)重新定义基类的虚函数方法，要求函数名，函数参数，返回类型完全相同.并\\$$且基于必须是虚函数，不能有static关键字,重写函数的访问修饰符可以与基类的不同。\\$$\qquad 基类指针指向派生类，若实现了重写，则调用派生类，若没，则调用基类,即实现多态$ 重定义$\qquad 子类(派生类)重新申明和定义基类的函数，要求函数名相同，但是返回值可以不同，参数\\$$不同，无论有无virtual，基类的都将被隐藏，参数相同，基类如果没有virtual，则基类的函被\\$$隐藏$ 重载$\qquad函数名相同，但是他们的参数列表个数或者顺序，类型不同，且不能仅有返回类型不同，要\\$$求再同一个作用于.$ 多态的实现方式概念$\qquad 多态: 即程序运行中，系统根据对象指针所指向的类别对相同的消息进行不同的方法处理$ 动态多态$\qquad 通过类的继承和虚函数机制，在程序运行期实现多态,虚函数表$ 静态多态$\qquad 函数重载；运算符重载$ 常用排序算法快速排序$\qquad 快速排序的实现:$12345678910111213141516171819void quickSort(int a[],int l ,int r)&#123;//或者vector if(l &lt; r)&#123; int i = l ,j = r ; int sed = a[i];//种子点 while(i &lt; j )&#123; while(i &lt; j &amp;&amp; a[j] &gt; sed ) --j; if(i &lt; j) a[i++] = a[j]; while(i &lt; j &amp;&amp; a[i] &lt; sed ) ++i; if(i &lt; j) a[j--] = a[i]; &#125; a[i] = sed; quickSort(a,l,i-1); qucikSort(a,i+1,r); &#125;&#125;]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic回归分析]]></title>
    <url>%2F2017%2F09%2F07%2Flogistic%2F</url>
    <content type="text"><![CDATA[Logistic回归分析$\qquad Logistic回归为概率型非线性回归模型，机器学习常用的二分类分类器，其表达式为:$ $\quad \quad z=w_{1}*x_{1}+w_{2}*x_{2}+\cdots +w_{n}*x_{n}+b=\sum_{i=0}^n w_{i}x_{i} (其中 b等于w_{0}，x_{0}等于1)则:$$$f(x) = \frac{1}{1+exp(-z)}$$ $\quad \quad$即对于二分类，如果$f(x)\ge{0.5}$,则$x$属于第一类，即预测$y=1$，反之$x$属于第二类，预测$y=0$；样本的分布如下，其中，$C_1$表示第一个类别，$C_2$表示第二个类别，样本个数为$n$ $$trainingData \quad\, x^1 \quad\, x^2 \quad\, x^3 \quad\,\cdots \quad\, x^n $$ $\qquad \qquad \qquad \qquad \qquad \qquad labels \qquad \quad C_{1} \quad C_{1} \quad C_{2} \quad \cdots \quad C_{1} \\$$\qquad$我们的目的是：对于类别为$1$的正样本$f_{w,b}(x)$ 尽可能大,而类别为$2$的负样本$f_{w,b}(x)$ 尽可能小,则我们需要最大化：$L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\cdots f_{w,b}(x^n)$来寻找最佳的$w$和$b$$$w^{*},b^{*} = arg\max\limits_{w,b}(L(w,b))\Longrightarrow w^{*},b^{*} = arg\min\limits_{w,b}(-ln{L(w,b)})$$ 随机梯度下降法$\qquad 我们需要优化的函数:-ln{L(w,b)} = -{ln{f_{w,b}(x^1)}+lnf_{w,b}(x^2)+ln(1-f_{w,b}(x^3))+\cdots lnf_{w,b}(x^n)}\quad \\$$$\qquad 假设：\begin{cases}\hat{y} = 1 \qquad x\in1 \\\\\\hat{y} = 0 \qquad x\in0\end{cases}\qquad 已知\,f(x) = \frac{1}{1+exp(-z)}\quad z = \sum_{i=0}^n w_{i}x_{i} 则$$$\qquad 我们需要优化的函数简化为：ln{L(w,b)} =\sum_{j=1}^{n}{\hat{y}^j\,lnf_{w,b}(x^j)+(1-\hat{y}^j)\,ln(1-f_{w,b}(x^j))} \\$ $\qquad 当\,\,\hat{y}=1时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = lnf_{w,b}(x) \\$$\qquad 当\,\,\hat{y}=0时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = ln(1-f_{w,b}(x)) \qquad \\$$\qquad 即均满足上式 , 因此:$ $\qquad \qquad \quad \frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\$ $\qquad \quad \quad 而 \, \frac{\partial lnf_{w,b}(x)}{\partial w_i}=\frac{\partial lnf_{w,b}(x)}{\partial z}*\frac{\partial z}{\partial w_i} \\$ $\qquad \qquad \qquad \qquad \quad=\frac{1}{f_{w,b}(x)}* \frac{\partial f_{w,b}(x)}{\partial z}*x_i \\$ $\qquad \qquad \qquad \qquad \quad=\frac{1}{f_{w,b}(x)}*f_{w,b}(x)*(1-f_{w,b}(x))*x_i \\$ $\qquad \qquad \qquad \qquad \quad=(1-f_{w,b}(x))*x_i \\$ $\quad \quad 同理 \quad \frac{\partial (1-lnf_{w,b}(x))}{\partial w_i}=f_{w,b}(x)*x_i \qquad 则化简后:\\$$\qquad \quad\,\, \qquad \frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\$ $\qquad \qquad \qquad \quad \qquad = \sum_{j=1}^{n}{\hat{y}^j(1-f_{w,b}(x^j))x^j_i+(1-\hat{y}^j)*f_{w,b}(x^j)x^j_i} \\$ $\qquad \qquad \quad\qquad \qquad = \sum_{j=1}^{n}(\hat{y}^j -f_{w,b}(x^j))x^j_i \\$ $\qquad b的推导与w的相似，可以得到w的更新迭代过程：w_{i} \leftarrow w_{i}-\alpha*\sum_{j=0}^{n}(\hat{y}^j-f_{w,b}(x^j))x^j_i \\$ 思考题1. 为什么选用$crossEntropy$损失函数，而不用L2损失函数$答:logistic不像linear \,\, regression使用L2损失函数的原因，主要是由于logistic的funcion的形式，\\$$由于sigmoid函数的存在，如果logistic采取L2 loss时，损失函数为：\\$$$\frac{\partial (f_{w,b}(x)-\hat{y})^2}{\partial w_i}=2(f_{w,b}(x)-\hat{y})f_{w,b}(x)(1-f_{w,b}(x))x_i $$$则当\,\hat{y}=1, f_{w,b}(x) = 1 \quad 预测为1 ，即预测完全正确时 \quad loss=0 \quad \\$$但是当\,\hat{y}=1,f_{w,b}(x) = 0 \quad 预测为0 ，即预测完全错误时 \quad loss却依然为0 \quad显然不对 \\$ 2. $logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数$答: 我们假设样本的分布服从二次高斯分布，即\\$ $f_{\mu,\Sigma}(x) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu)^T|\Sigma|^{-1}(x-\mu)},其中\mu为均值，\Sigma为协方差矩阵 \\$ $输入为x，输出f_{\mu,\Sigma}(x)为样本x的概率密度，高斯分布的形状分布取决于均值\mu和协方差矩阵\Sigma, \\$$因此需要求取最佳的高斯分布来满足样本的分布 \\$ $$Maximum Likelihood : L(\mu,\Sigma) = f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)f_{\mu,\Sigma}(x^3)\cdots\cdots f_{\mu,\Sigma}(x^{N})$$$$\mu^{*}，\Sigma^{*} = arg\max\limits_{\mu,\Sigma}L(\mu,\Sigma)$$$$\mu^{*} = \frac{1}{N}\sum_{i=0}^{N}{x^i}$$$$\Sigma^{*} = \frac{1}{N}\sum_{i=0}^{N}{(x^i-\mu^{*})(x^i-\mu^{*})^T}$$ $对于一个二分类，我们假设类别1的样本高斯分布的均值为\mu^1,类别2的样本的高斯分布均值为\mu^2,他们具有相同的协方差\Sigma \\$$$\mu^1 = \sum_{i=1}^{n_1} x_i\qquad (x_i \in C_1) \quad ;\quad \mu^2 = \sum_{i=1}^{n_2} x_i\quad(x_i \in C_2) $$$$\Sigma^1 = \sum_{i=1}^{n_1}(x_i-u^1)(x_i-u^1)^T ;\quad \Sigma^2 = \sum_{i=1}^{n_2}(x_i-u^2)(x_i-u^2)^T ;\quad \Sigma=\frac{n_1}{n_1+n_2}\Sigma^1+\frac{n_1}{n_1+n_2}\Sigma^2 $$ $对于样本x，如果属于C_1则有：\\$ $\qquad \qquad\qquad \qquad P(C_{1}|x) \,\,= \frac{P(C_{1},x)}{P(x)} \\$ $\qquad \qquad\qquad \qquad \qquad \qquad =\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{1})*P(C_{1})+P(x|C_{2})*P(C_{2})} \\$ $\qquad \qquad\qquad \qquad \qquad \qquad =\frac{1}{1+\frac{P(x|C_{2})P(C_{2})}{P(x|C_{1})P(C_{1})}} \\$ $\qquad \qquad\qquad \qquad \qquad \qquad =\frac{1}{1+exp(-\alpha)} \\$ $其中\,\, \alpha= \ln(\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{2})*P(C_{2})})$ $将P(x|C_i)带入高斯分布的公式:\\$$$P(C_1)=\frac{n_1}{n_1+n_2}\quad , \quad P(C_2)=\frac{n_2}{n_1+n_2} $$$$P(x|C_1) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)} $$$$P(x|C_2) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp{-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2)} $$$\alpha= lnP(x|C_1)-lnP(x|C_2)+ln\frac{P(C_1)}{P(C_2)} \\$$\quad =-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)-(-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2))+ln\frac{n_1}{n_2}\\$$\quad =-\frac{1}{2}x^T(\Sigma)^{-1}x+(u^1)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}x^T(\Sigma)^{-1}x-(u^2)^T(\Sigma)^{-1}x+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$$\quad = (u^1-u^2)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$$\quad = wx+b\\$$\quad w = (u^1-u^2)^T(\Sigma)^{-1} \quad ; \quad b=-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\$$\quad 因此可以得到对于满足猜想的二次高斯分布的datasets，生成模型的分类表达式与logistic是一致的 \\$ 生成模型与判别模型生成模型基于现有的样本，对样本分布做了一个猜测（极大似然），因此当数据集较少，或者有噪声的时候， 都能达到一个较好的结果(不过分依赖于实际样本),并且可以根据不同的概率model完成样本分布的gauss 判别模型基于决策的方式（判别式），通过优化方法(sgd)寻找最优参数，对样本的依赖大，样本充足时，其 效果一般比生成模型好(基于事实 not 基于猜测) 小扩展多分类基于先验概率得出的每个类别的后验概率为softmax函数，即： $\\$$\qquad \qquad \qquad \qquad \, P(C_i|x) = \frac{P(x|C_i)P(C_i)}{\sum_{j=1}^{n}P(x|C_j)P(C_j)}\\$ $\qquad \qquad \qquad \qquad \qquad \qquad = \frac{exp(a_k)}{\sum_{j=1}^{n}a_j}\\$ 待续未完待续]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>生成模型</tag>
      </tags>
  </entry>
</search>
